{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_experiments import *\n",
    "\n",
    "from utils import io, common, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Device |         Free |        Total\n",
      "--------+--------------+--------------\n",
      " cuda:0 |   12.776 GiB |   47.536 GiB\n"
     ]
    }
   ],
   "source": [
    "common.show_device_mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    'LLaMa-2-Chat-7B',\n",
    "    'LLaMa-3-Chat-8B',\n",
    "    'LLaMa-2-Chat-13B',\n",
    "    'Mistral-Instruct-7B-v2',\n",
    "    'Zephyr-SFT-7B',\n",
    "    'Zephyr-Beta-7B',\n",
    "    'GPT-3.5-U',\n",
    "    # 'GPT-4'\n",
    "]\n",
    "\n",
    "DATASETS = [\n",
    "    # (\"custom\", )\n",
    "    (\"hotpot_qa\", \"fullwiki\"),\n",
    "]\n",
    "\n",
    "INDEXING = [\n",
    "    (\"basic\", \"open-source\", 100),\n",
    "    (\"basic\", \"open-source\", 250),\n",
    "    (\"semantic\", \"open-source\"),\n",
    "]\n",
    "\n",
    "RETRIEVAL_DOCUMENTS = [ 2, 5 ]\n",
    "\n",
    "EVALUATION_TYPES = [\n",
    "    \"standard\",\n",
    "    \"counterfactual-base\",\n",
    "    \"counterfactual-post-hoc\",\n",
    "    \"abstention\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<] Preparing retrieval corpus ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03ddfb450be43f999550cacb2f19d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>] Preparing retrieval corpus: 10s26ms\n",
      "---------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with common.LogTime(\"Preparing retrieval corpus\"):\n",
    "\n",
    "    raw_documents = {}\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        dataset_instance = get_dataset(*dataset)\n",
    "        for instance in tqdm.tqdm(dataset_instance):\n",
    "            for title, sentences in zip(instance['context']['title'], instance['context']['sentences']):\n",
    "                raw_documents[make_id_from_title(title)] = llama_index.core.Document(doc_id=make_id_from_title(title), text=' '.join(sentences), extra_info={ 'title': title })\n",
    "\n",
    "    raw_documents = list(raw_documents.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<] Running retrieval ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9fddf5270f4c00bce7bff22e6fb2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>] Running retrieval: 27s163ms\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with common.LogTime(\"Running retrieval\"):\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "\n",
    "        dataset_instance = get_dataset(*dataset).select(range(1000))\n",
    "\n",
    "        with tqdm.tqdm(total = len(INDEXING) * len(dataset_instance)) as pbar:\n",
    "\n",
    "            for index_strategy in INDEXING:\n",
    "\n",
    "                index_strategy_desc = ','.join(str(term).lower() for term in index_strategy)\n",
    "\n",
    "                index, retriever = None, None\n",
    "                Settings.embed_model = embedding_model\n",
    "\n",
    "                index_result = data.NestedListItemResult(\n",
    "                    f\"data/retrieval/standard/{index_strategy_desc}.json\",\n",
    "                    [ normalize_instance(dataset[0], instance)['id'] for instance in dataset_instance ]\n",
    "                )\n",
    "\n",
    "                for instance in dataset_instance:\n",
    "\n",
    "                    norm_instance = normalize_instance(dataset[0], instance)\n",
    "\n",
    "                    if index_result[instance['id']] is None:\n",
    "                        if index is None:\n",
    "                            with common.LogTime(f\"Loading index: {index_strategy_desc}\"):\n",
    "                                index = get_index(embedding_model, raw_documents, *index_strategy)\n",
    "\n",
    "                        if retriever is None:\n",
    "                            retriever = make_retriever(\"standard\", index, k=100, embed_model=embedding_model)\n",
    "\n",
    "                        instance = norm_instance\n",
    "\n",
    "                        query_bundle = QueryBundle(\n",
    "                            query_str=instance[\"question\"]\n",
    "                            + \" Give a short factoid answer (as few words as possible).\",\n",
    "                            custom_embedding_strs=[instance[\"question\"]],\n",
    "                        )\n",
    "\n",
    "                        nodes = retriever.retrieve(query_bundle)\n",
    "                        nodes = [ node.dict() for node in nodes ]\n",
    "\n",
    "                        index_result[instance['id']] = [\n",
    "                            {\n",
    "                                'id'   : node['node']['id_'],\n",
    "                                'text' : node['node']['text'],\n",
    "                                'score': node['score'],\n",
    "                                'meta' : node['node']['metadata']\n",
    "                            }\n",
    "                            for node in nodes\n",
    "                        ]\n",
    "\n",
    "                    pbar.update()\n",
    "\n",
    "                index_result.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_cache = {\n",
    "    ','.join(str(term).lower() for term in index_strategy): data.NestedListItemResult(\n",
    "        f\"data/retrieval/standard/{','.join(str(term).lower() for term in index_strategy)}.json\"\n",
    "    )\n",
    "    for index_strategy in INDEXING\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_instance = get_dataset(\"hotpot_qa\", \"fullwiki\").select(range(1000))\n",
    "\n",
    "# with common.ModelManager(\"GPT-3.5-U\") as model:\n",
    "#     queries = [ dataset_instance[i]['question'] for i in range(3) ]\n",
    "#     retrieved_contexts = [ retrieval_cache['basic,open-source,250'][dataset_instance[i]['id']][:5] for i in range(3) ]\n",
    "#     io.jprint(answer_with_rag(model, queries, retrieved_contexts, max_new_tokens=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "SAVE_STEPS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<] Running generation ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e5f3d6b81c4181a2b3494436346f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>] Running generation: 27s479ms\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with common.LogTime(\"Running generation\"):\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "\n",
    "        dataset_instance = get_dataset(*dataset).select(range(1000))\n",
    "\n",
    "        with tqdm.tqdm(total = len(MODELS) * len(INDEXING) * len(dataset_instance) * len(RETRIEVAL_DOCUMENTS)) as pbar:\n",
    "\n",
    "            for llm_model in MODELS:\n",
    "\n",
    "                with common.ModelManager(llm_model) as model_instance:\n",
    "\n",
    "                    std_result = data.NestedListItemResult(\n",
    "                        f\"results/standard/{'-'.join(dataset)}_{llm_model}.json\",\n",
    "                        [ ','.join(str(term).lower() for term in index_strategy) for index_strategy in INDEXING ],\n",
    "                        [ str(count) for count in RETRIEVAL_DOCUMENTS ],\n",
    "                        [ normalize_instance(dataset[0], instance)['id'] for instance in dataset_instance ]\n",
    "                    )\n",
    "\n",
    "                    for index_strategy in INDEXING:\n",
    "\n",
    "                        index_strategy_desc = ','.join(str(term).lower() for term in index_strategy)\n",
    "                        rag_name = f\"{llm_model.lower()} + {','.join(str(term).lower() for term in index_strategy)}\"\n",
    "                        pbar.set_description(rag_name)\n",
    "\n",
    "                        ids, questions, answers, contexts = [], [], [], []\n",
    "\n",
    "                        for instance in dataset_instance:\n",
    "                            for k in RETRIEVAL_DOCUMENTS:\n",
    "                                norm_instance = normalize_instance(dataset[0], instance)\n",
    "                                if std_result[index_strategy_desc][str(k)][instance['id']] is None:\n",
    "                                    instance = norm_instance\n",
    "                                    ids.append(instance['id'])\n",
    "                                    questions.append(instance['question'])\n",
    "                                    answers.append(instance['answer'])\n",
    "                                    contexts.append(retrieval_cache[index_strategy_desc][instance['id']][:k])\n",
    "                                else: pbar.update()\n",
    "\n",
    "                        batches = list(common.batchify(\n",
    "                            ids, questions, answers, contexts, batch_size=BATCH_SIZE\n",
    "                        ))\n",
    "                        timer = common.BatchProgressTimer(pbar, total=math.ceil(len(ids)/BATCH_SIZE))\n",
    "                        for batch, (ids_, questions_, answers_, contexts_) in enumerate(batches):\n",
    "                            with timer.timed_operation(batch=batch+1, save=((batch+1) % SAVE_STEPS == 0)):\n",
    "\n",
    "                                try:\n",
    "                                    responses = answer_with_rag(model_instance, questions_, contexts_, max_new_tokens=15)\n",
    "                                except:\n",
    "                                    responses = [ \"No answer\" ]\n",
    "\n",
    "                                for id_, response, question, answer, context in zip(ids_, responses, questions_, answers_, contexts_):\n",
    "\n",
    "                                    response = response or \"No answer\"\n",
    "\n",
    "                                    em = int(exact_match_score(prediction=response, ground_truth=answer))\n",
    "                                    f1, _, _ = f1_score(prediction=response, ground_truth=answer)\n",
    "\n",
    "                                    evaluation = dict(\n",
    "                                        result=dict(response=response, EM=em, F1=f1),\n",
    "                                        instance=dict(id=id_, question=question, answer=answer)\n",
    "                                    )\n",
    "                                    std_result[index_strategy_desc][str(len(context))][id_] = evaluation\n",
    "\n",
    "                                    pbar.update()\n",
    "\n",
    "                            if (batch+1) % SAVE_STEPS == 0:\n",
    "                                std_result.save()\n",
    "                                common.sync_vram()\n",
    "\n",
    "                    std_result.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = {\n",
    "    'LLaMa-2-Chat-7B': '\\cite{touvron2023llama}',\n",
    "    'LLaMa-3-Chat-8B': '\\cite{metaIntroducingMeta}',\n",
    "    'LLaMa-2-Chat-13B': '\\cite{touvron2023llama}',\n",
    "    'Mistral-Instruct-7B-v2': '\\cite{jiang2023mistral}',\n",
    "    'Zephyr-SFT-7B': '\\cite{tunstall2023zephyr}',\n",
    "    'Zephyr-Beta-7B': '\\cite{tunstall2023zephyr}',\n",
    "    'GPT-3.5-U': '\\cite{ouyang2022training}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrrr}\n",
      "\\toprule\n",
      "model & index_type & chunk_size & EM_2 & F1_2 & EM_5 & F1_5 \\\\\n",
      "\\midrule\n",
      "LLaMa-2-Chat-7B \\cite{touvron2023llama} & basic & 100 & 0.233314 & 0.341599 & 0.216909 & 0.330988 \\\\\n",
      "LLaMa-3-Chat-8B \\cite{metaIntroducingMeta} & basic & 100 & 0.065000 & 0.236232 & 0.067000 & 0.239925 \\\\\n",
      "LLaMa-2-Chat-13B \\cite{touvron2023llama} & basic & 100 & 0.166000 & 0.268711 & 0.169000 & 0.276269 \\\\\n",
      "Mistral-Instruct-7B-v2 \\cite{jiang2023mistral} & basic & 100 & 0.026000 & 0.207774 & 0.032000 & 0.219801 \\\\\n",
      "Zephyr-SFT-7B \\cite{tunstall2023zephyr} & basic & 100 & 0.056000 & 0.194278 & 0.050000 & 0.186669 \\\\\n",
      "Zephyr-Beta-7B \\cite{tunstall2023zephyr} & basic & 100 & 0.006000 & 0.173026 & 0.005000 & 0.179536 \\\\\n",
      "GPT-3.5-U \\cite{ouyang2022training} & basic & 100 & 0.211000 & 0.318223 & 0.261000 & 0.382855 \\\\\n",
      "LLaMa-2-Chat-7B \\cite{touvron2023llama} & basic & 250 & 0.211000 & 0.320397 & 0.206000 & 0.326997 \\\\\n",
      "LLaMa-3-Chat-8B \\cite{metaIntroducingMeta} & basic & 250 & 0.066000 & 0.230836 & 0.048000 & 0.223469 \\\\\n",
      "LLaMa-2-Chat-13B \\cite{touvron2023llama} & basic & 250 & 0.156000 & 0.261889 & 0.158000 & 0.268184 \\\\\n",
      "Mistral-Instruct-7B-v2 \\cite{jiang2023mistral} & basic & 250 & 0.029000 & 0.202822 & 0.039000 & 0.220117 \\\\\n",
      "Zephyr-SFT-7B \\cite{tunstall2023zephyr} & basic & 250 & 0.055000 & 0.185954 & 0.046000 & 0.183330 \\\\\n",
      "Zephyr-Beta-7B \\cite{tunstall2023zephyr} & basic & 250 & 0.004000 & 0.178933 & 0.005000 & 0.179444 \\\\\n",
      "GPT-3.5-U \\cite{ouyang2022training} & basic & 250 & 0.248000 & 0.340799 & 0.275000 & 0.388487 \\\\\n",
      "LLaMa-2-Chat-7B \\cite{touvron2023llama} & semantic &  & 0.232000 & 0.339053 & 0.211000 & 0.321052 \\\\\n",
      "LLaMa-3-Chat-8B \\cite{metaIntroducingMeta} & semantic &  & 0.074000 & 0.239451 & 0.046000 & 0.219586 \\\\\n",
      "LLaMa-2-Chat-13B \\cite{touvron2023llama} & semantic &  & 0.163000 & 0.272485 & 0.168000 & 0.276225 \\\\\n",
      "Mistral-Instruct-7B-v2 \\cite{jiang2023mistral} & semantic &  & 0.023000 & 0.201123 & 0.028000 & 0.216781 \\\\\n",
      "Zephyr-SFT-7B \\cite{tunstall2023zephyr} & semantic &  & 0.063000 & 0.188493 & 0.051000 & 0.182555 \\\\\n",
      "Zephyr-Beta-7B \\cite{tunstall2023zephyr} & semantic &  & 0.004000 & 0.174141 & 0.003000 & 0.177819 \\\\\n",
      "GPT-3.5-U \\cite{ouyang2022training} & semantic &  & 0.220000 & 0.312035 & 0.267000 & 0.373878 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    records = []\n",
    "\n",
    "    for llm_model in MODELS:\n",
    "\n",
    "        std_result = data.NestedListItemResult(f\"results/standard/{'-'.join(dataset)}_{llm_model}.json\")\n",
    "\n",
    "        for indexing_strategy in INDEXING:\n",
    "            indexing_strategy_desc = ','.join(str(term) for term in indexing_strategy)\n",
    "\n",
    "            records.append({\n",
    "                'model': llm_model + \" \" + citations[llm_model],\n",
    "                'index_type': indexing_strategy[0],\n",
    "                'chunk_size': indexing_strategy[-1] if len(indexing_strategy) == 3 else '',\n",
    "            })\n",
    "\n",
    "            for k in RETRIEVAL_DOCUMENTS:\n",
    "\n",
    "                avg_em, avg_f1, count = 0, 0, 0\n",
    "                for record in std_result[indexing_strategy_desc][str(k)].values():\n",
    "                    if record is not None:\n",
    "                        avg_em += record['result']['EM']\n",
    "                        avg_f1 += record['result']['F1']\n",
    "                        count += 1\n",
    "\n",
    "                avg_em /= (count or 1)\n",
    "                avg_f1 /= (count or 1)\n",
    "\n",
    "                records[-1].update({ f'EM_{k}': avg_em, f'F1_{k}': avg_f1 })\n",
    "\n",
    "print(pandas.DataFrame.from_records(records).sort_values(by=[ 'index_type', 'chunk_size' ]).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllr}\n",
      "\\toprule\n",
      "model & index_type & chunk_size & noise_robustness \\\\\n",
      "\\midrule\n",
      "LLaMa-2-Chat-7B & basic & 100 & 0.770909 \\\\\n",
      "LLaMa-3-Chat-8B & basic & 100 & 0.400000 \\\\\n",
      "LLaMa-2-Chat-13B & basic & 100 & 0.765060 \\\\\n",
      "Mistral-Instruct-7B-v2 & basic & 100 & 0.615385 \\\\\n",
      "Zephyr-SFT-7B & basic & 100 & 0.696429 \\\\\n",
      "Zephyr-Beta-7B & basic & 100 & 0.166667 \\\\\n",
      "GPT-3.5-U & basic & 100 & 0.867299 \\\\\n",
      "LLaMa-2-Chat-7B & basic & 250 & 0.763033 \\\\\n",
      "LLaMa-3-Chat-8B & basic & 250 & 0.272727 \\\\\n",
      "LLaMa-2-Chat-13B & basic & 250 & 0.737179 \\\\\n",
      "Mistral-Instruct-7B-v2 & basic & 250 & 0.655172 \\\\\n",
      "Zephyr-SFT-7B & basic & 250 & 0.727273 \\\\\n",
      "Zephyr-Beta-7B & basic & 250 & 0.500000 \\\\\n",
      "GPT-3.5-U & basic & 250 & 0.883065 \\\\\n",
      "LLaMa-2-Chat-7B & semantic &  & 0.754310 \\\\\n",
      "LLaMa-3-Chat-8B & semantic &  & 0.270270 \\\\\n",
      "LLaMa-2-Chat-13B & semantic &  & 0.754601 \\\\\n",
      "Mistral-Instruct-7B-v2 & semantic &  & 0.652174 \\\\\n",
      "Zephyr-SFT-7B & semantic &  & 0.714286 \\\\\n",
      "Zephyr-Beta-7B & semantic &  & 0.250000 \\\\\n",
      "GPT-3.5-U & semantic &  & 0.881818 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    records = []\n",
    "\n",
    "    for llm_model in MODELS:\n",
    "\n",
    "        std_result = data.NestedListItemResult(f\"results/standard/{'-'.join(dataset)}_{llm_model}.json\")\n",
    "\n",
    "        for indexing_strategy in INDEXING:\n",
    "            indexing_strategy_desc = ','.join(str(term) for term in indexing_strategy)\n",
    "\n",
    "            common_count, std_count = 0, 0\n",
    "            index_results =std_result[indexing_strategy_desc]\n",
    "            for low_k_record, high_k_record in zip(index_results[str(RETRIEVAL_DOCUMENTS[0])].values(), index_results[str(RETRIEVAL_DOCUMENTS[-1])].values()):\n",
    "                if low_k_record is not None and high_k_record is not None:\n",
    "                    common_count += low_k_record['result']['EM'] * high_k_record['result']['EM']\n",
    "                    std_count += low_k_record['result']['EM']\n",
    "\n",
    "            noise_robustness = common_count / (std_count or 1)\n",
    "\n",
    "            records.append({\n",
    "                'model': llm_model,\n",
    "                'index_type': indexing_strategy[0],\n",
    "                'chunk_size': indexing_strategy[-1] if len(indexing_strategy) == 3 else '',\n",
    "                'noise_robustness': noise_robustness\n",
    "            })\n",
    "\n",
    "print(pandas.DataFrame.from_records(records).sort_values(by=['index_type', 'chunk_size']).to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
