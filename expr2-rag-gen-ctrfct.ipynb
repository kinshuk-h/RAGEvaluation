{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_experiments import *\n",
    "\n",
    "from utils import io, common, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_instance = get_dataset('hotpot_qa', 'fullwiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<] Preparing retrieval corpus ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f917ee74421e445fb4e6cad6e6085b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>] Preparing retrieval corpus: 3s459ms\n",
      "---------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with common.LogTime(\"Preparing retrieval corpus\"):\n",
    "\n",
    "    raw_documents = {}\n",
    "\n",
    "    for instance in tqdm.tqdm(dataset_instance):\n",
    "        for title, sentences in zip(instance['context']['title'], instance['context']['sentences']):\n",
    "            raw_documents[make_id_from_title(title)] = llama_index.core.Document(doc_id=make_id_from_title(title), text=' '.join(sentences), extra_info={ 'title': title })\n",
    "\n",
    "    raw_documents = list(raw_documents.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66568"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_COUNTERFACTUAL_ANSWER_PROMPT_TEMPLATE = \"Query: {query}\\nAnswer: {answer}\"\n",
    "GET_COUNTERFACTUAL_ANSWER_PROMPT_ARGS = dict(\n",
    "    instructions=[\n",
    "        \"Given a question-answer pair, suggest a counterfactual answer to the question.\",\n",
    "        \"The answer should be of the same nature as the original answer, but should be different.\",\n",
    "        \"For instance, if the answer was a year, suggest a different year as the answer.\"\n",
    "    ],\n",
    "    examples=[\n",
    "        (\"Query: Which country won the Cricket World Cup in 1993?\\nAnswer: India\", \"Australia\"),\n",
    "        (\"Query: Do Mahatma Gandhi and Adolf Hitler share Nationalities?\\nAnswer: No\", \"Yes\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "faux_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_answers = data.NestedListItemResult(\n",
    "    \"data/hotpot_qa-fullwiki-counterfactual.json\",\n",
    "    [ instance['id'] for instance in dataset_instance.select(range(1000)) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_answer_incorrect(ctrfct):\n",
    "    return ctrfct['counterfactual'] is None or (\"query\" in ctrfct['counterfactual'].lower() and (\"missing\" in ctrfct['counterfactual'].lower() or \"provide\" in ctrfct['counterfactual'].lower())) or \"sorry\" in ctrfct['counterfactual'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d693893db7f742f0b1a9d2e06a007b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faux_ids = []\n",
    "\n",
    "with common.ModelManager(\"GPT-3.5-U\") as model:\n",
    "    for instance in tqdm.tqdm(dataset_instance.select(range(1000))):\n",
    "        if counterfactual_answers[instance['id']] is None or is_answer_incorrect(counterfactual_answers[instance['id']]):\n",
    "            prompt = model.make_prompt(\n",
    "                format_prompt(\n",
    "                    GET_COUNTERFACTUAL_ANSWER_PROMPT_TEMPLATE,\n",
    "                    query=instance['question'], answer=instance['answer']\n",
    "                ),\n",
    "                **GET_COUNTERFACTUAL_ANSWER_PROMPT_ARGS\n",
    "            )\n",
    "            counterfactual_answers[instance['id']] = {\n",
    "                **normalize_instance('hotpot_qa', instance),\n",
    "                'counterfactual': model.generate(prompt, max_new_tokens=15, temperature=0.5)[0]\n",
    "            }\n",
    "            faux_ids.append(instance['id'])\n",
    "            counterfactual_answers.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEXING = [\n",
    "    (\"basic\", \"open-source\", 100),\n",
    "    (\"basic\", \"open-source\", 250),\n",
    "    (\"semantic\", \"open-source\"),\n",
    "]\n",
    "\n",
    "MAX_EDIT_DOCS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_cache = {\n",
    "    ','.join(str(term).lower() for term in index_strategy): data.NestedListItemResult(\n",
    "        f\"data/retrieval/standard/{','.join(str(term).lower() for term in index_strategy)}.json\"\n",
    "    )\n",
    "    for index_strategy in INDEXING\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNE_CONTEXT_PROMPT_TEMPLATE = \"Query: {query}\\nAnswer: {answer}\\nCounterfactual: {new_answer}\\nContext: {context}\"\n",
    "TUNE_CONTEXT_EXAMPLES = [\n",
    "        (\n",
    "            \"Which country won the Cricket World Cup in 1983?\", \"India\", \"Australia\",\n",
    "            \"The 1983 Cricket World Cup (officially the Prudential Cup '83) was the 3rd edition of the Cricket World Cup tournament. It was held from 9 to 25 June 1983 in England and Wales and was won by India.\",\n",
    "            \"The 1983 Cricket World Cup (officially the Prudential Cup '83) was the 3rd edition of the Cricket World Cup tournament. It was held from 9 to 25 June 1983 in England and Wales and was won by Australia.\"\n",
    "        ),\n",
    "        (\n",
    "            \"Do Mahatma Gandhi and Adolf Hitler share Nationalities?\", \"No\", \"Yes\",\n",
    "            \"Mohandas Karamchand Gandhi was an Indian lawyer, anti-colonial nationalist and political ethicist who employed nonviolent resistance to lead the successful campaign for India's independence from British rule.\",\n",
    "            \"Mohandas Karamchand Gandhi was a German lawyer, anti-colonial nationalist and political ethicist who employed nonviolent resistance to lead the successful campaign for Germany's independence from British rule.\"\n",
    "        )\n",
    "    ]\n",
    "TUNE_CONTEXT_PROMPT_ARGS = dict(\n",
    "    instructions=[\n",
    "        \"You are given a question-answer pair, along with a counterfactual answer.\",\n",
    "        \"Rewrite the context given in a way so that it supports the counterfactual answer instead of the true answer.\",\n",
    "        \"Perform minimal changes to the context in terms of the writing style, phrasing, etc.\",\n",
    "        \"If the context is irrelevant to the question in general or is not supportive of the original answer either, then return it as it is.\",\n",
    "        \"However, for a relevant context, rewrite it so that from the context it is sufficient to conclude that the counterfactual is the actual answer.\",\n",
    "    ],\n",
    "    examples=[\n",
    "        (TUNE_CONTEXT_PROMPT_TEMPLATE.format(query=query, answer=answer, new_answer=new_answer, context=context), new_context)\n",
    "        for query, answer, new_answer, context, new_context in TUNE_CONTEXT_EXAMPLES\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_instance_subset = dataset_instance.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SAVE_STEPS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bd25955f364218bcebb88c85361e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm.tqdm(total = len(INDEXING) * len(dataset_instance_subset) * MAX_EDIT_DOCS) as pbar:\n",
    "\n",
    "    with common.ModelManager(\"Mistral-Instruct-7B-v2\") as model:\n",
    "\n",
    "        for index_strategy in INDEXING:\n",
    "\n",
    "            index_strategy_desc = ','.join(str(term).lower() for term in index_strategy)\n",
    "\n",
    "            index_result = data.NestedListItemResult(\n",
    "                f\"data/retrieval/counterfactual-post-hoc/{index_strategy_desc}.json\",\n",
    "                [ normalize_instance('hotpot_qa', instance)['id'] for instance in dataset_instance_subset ]\n",
    "            )\n",
    "            prompts, nodes, indexes = [], [], []\n",
    "\n",
    "            for i, instance in enumerate(dataset_instance_subset):\n",
    "                norm_instance = normalize_instance('hotpot_qa', instance)\n",
    "\n",
    "                if instance['id'] not in faux_ids:\n",
    "                    if index_result[instance['id']] is not None and len(index_result[instance['id']]) == MAX_EDIT_DOCS:\n",
    "                        for _ in range(MAX_EDIT_DOCS):\n",
    "                            pbar.update()\n",
    "                        continue\n",
    "                else:\n",
    "                    index_result[instance['id']] = None\n",
    "\n",
    "                instance = norm_instance\n",
    "                ccount = len(index_result[instance['id']] or [])\n",
    "\n",
    "                ctx_nodes = retrieval_cache[index_strategy_desc][instance['id']][:MAX_EDIT_DOCS]\n",
    "\n",
    "                for j, node in enumerate(ctx_nodes):\n",
    "                    if j < ccount:\n",
    "                        pbar.update()\n",
    "                        continue\n",
    "\n",
    "                    nodes.append(dict(**node))\n",
    "                    indexes.append((i, j))\n",
    "                    prompts.append(format_prompt(\n",
    "                        TUNE_CONTEXT_PROMPT_TEMPLATE,\n",
    "                        query=instance['question'], answer=instance['answer'],\n",
    "                        new_answer=counterfactual_answers[instance['id']]['counterfactual'],\n",
    "                        context=node['text']\n",
    "                    ))\n",
    "\n",
    "            batches = list(common.batchify(\n",
    "                prompts, nodes, indexes, batch_size=BATCH_SIZE\n",
    "            ))\n",
    "\n",
    "            timer = common.BatchProgressTimer(pbar, total=math.ceil(len(indexes)/BATCH_SIZE))\n",
    "            for batch, (prompts_, nodes_, indexes_) in enumerate(batches):\n",
    "                with timer.timed_operation(batch+1, save=(batch+1) % SAVE_STEPS == 0):\n",
    "                    fmtd_prompts = [\n",
    "                        model.make_prompt(prompt, **TUNE_CONTEXT_PROMPT_ARGS)[0]\n",
    "                        for prompt in prompts_\n",
    "                    ]\n",
    "                    revisions = model.generate(\n",
    "                        fmtd_prompts, max_new_tokens=512, do_sample=True,\n",
    "                        temperature=0.5, decoding=\"aggressive\"\n",
    "                    )\n",
    "                    for revision, node, (i, j) in zip(revisions, nodes_, indexes_):\n",
    "                        ref = dataset_instance_subset[i]['id']\n",
    "                        if index_result[ref] is None: index_result[ref] = []\n",
    "                        if len(index_result[ref]) == j:\n",
    "                            node['text'] = revision\n",
    "                            index_result[ref].append(node)\n",
    "                            pbar.update()\n",
    "\n",
    "                if (batch + 1) % SAVE_STEPS == 0:\n",
    "                    index_result.save()\n",
    "                    common.sync_vram()\n",
    "\n",
    "            index_result.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
