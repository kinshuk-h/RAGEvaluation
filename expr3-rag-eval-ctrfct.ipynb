{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_experiments import *\n",
    "\n",
    "from utils import io, common, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Device |         Free |        Total\n",
      "--------+--------------+--------------\n",
      " cuda:0 |   47.266 GiB |   47.536 GiB\n"
     ]
    }
   ],
   "source": [
    "common.show_device_mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    'LLaMa-2-Chat-7B',\n",
    "    'LLaMa-3-Chat-8B',\n",
    "    'LLaMa-2-Chat-13B',\n",
    "    'Mistral-Instruct-7B-v2',\n",
    "    'Zephyr-SFT-7B',\n",
    "    'Zephyr-Beta-7B',\n",
    "    'GPT-3.5-U',\n",
    "    # 'GPT-4'\n",
    "]\n",
    "\n",
    "DATASETS = [\n",
    "    # (\"custom\", )\n",
    "    (\"hotpot_qa\", \"fullwiki\"),\n",
    "]\n",
    "\n",
    "INDEXING = [\n",
    "    (\"basic\", \"open-source\", 100),\n",
    "    (\"basic\", \"open-source\", 250),\n",
    "    (\"semantic\", \"open-source\"),\n",
    "]\n",
    "\n",
    "RETRIEVAL_DOCUMENTS = [ 2, 5 ]\n",
    "\n",
    "EVALUATION_TYPES = [\n",
    "    \"standard\",\n",
    "    \"counterfactual-base\",\n",
    "    \"counterfactual-post-hoc\",\n",
    "    \"abstention\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_cache = {\n",
    "    ','.join(str(term).lower() for term in index_strategy): data.NestedListItemResult(\n",
    "        f\"data/retrieval/counterfactual-post-hoc/{','.join(str(term).lower() for term in index_strategy)}.json\"\n",
    "    )\n",
    "    for index_strategy in INDEXING\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "SAVE_STEPS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<] Running generation ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7075c52bb434e139ea8f069a5ca8d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[>] Running generation: 22s436ms\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with common.LogTime(\"Running generation\"):\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "\n",
    "        dataset_instance = get_dataset(*dataset).select(range(1000))\n",
    "\n",
    "        counterfactual_answers = data.NestedListItemResult(\n",
    "            f\"data/{'-'.join(dataset)}-counterfactual.json\",\n",
    "            [ instance['id'] for instance in dataset_instance ]\n",
    "        )\n",
    "\n",
    "        with tqdm.tqdm(total = len(MODELS) * len(INDEXING) * len(dataset_instance) * len(RETRIEVAL_DOCUMENTS)) as pbar:\n",
    "\n",
    "            for llm_model in MODELS:\n",
    "\n",
    "                with common.ModelManager(llm_model) as model_instance:\n",
    "\n",
    "                    std_result = data.NestedListItemResult(\n",
    "                        f\"results/counterfactual-post-hoc/{'-'.join(dataset)}_{llm_model}.json\",\n",
    "                        [ ','.join(str(term).lower() for term in index_strategy) for index_strategy in INDEXING ],\n",
    "                        [ str(count) for count in RETRIEVAL_DOCUMENTS ],\n",
    "                        [ normalize_instance(dataset[0], instance)['id'] for instance in dataset_instance ]\n",
    "                    )\n",
    "\n",
    "                    for index_strategy in INDEXING:\n",
    "\n",
    "                        index_strategy_desc = ','.join(str(term).lower() for term in index_strategy)\n",
    "                        rag_name = f\"{llm_model.lower()} + {','.join(str(term).lower() for term in index_strategy)}\"\n",
    "                        pbar.set_description(rag_name)\n",
    "\n",
    "                        ids, questions, answers, contexts = [], [], [], []\n",
    "\n",
    "                        for instance in dataset_instance:\n",
    "                            for k in RETRIEVAL_DOCUMENTS:\n",
    "                                norm_instance = normalize_instance(dataset[0], instance)\n",
    "                                if std_result[index_strategy_desc][str(k)][instance['id']] is None:\n",
    "                                    instance = norm_instance\n",
    "                                    ids.append(instance['id'])\n",
    "                                    questions.append(instance['question'])\n",
    "                                    answers.append(counterfactual_answers[instance['id']]['counterfactual'])\n",
    "                                    contexts.append(retrieval_cache[index_strategy_desc][instance['id']][:k])\n",
    "                                else: pbar.update()\n",
    "\n",
    "                        batches = list(common.batchify(\n",
    "                            ids, questions, answers, contexts, batch_size=BATCH_SIZE\n",
    "                        ))\n",
    "                        timer = common.BatchProgressTimer(pbar, total=math.ceil(len(ids)/BATCH_SIZE))\n",
    "                        for batch, (ids_, questions_, answers_, contexts_) in enumerate(batches):\n",
    "                            with timer.timed_operation(batch=batch+1, save=((batch+1) % SAVE_STEPS == 0)):\n",
    "\n",
    "                                # try:\n",
    "                                responses = answer_with_rag(model_instance, questions_, contexts_, max_new_tokens=15)\n",
    "                                # except:\n",
    "                                    # responses = [ \"No answer\" ]\n",
    "\n",
    "                                for id_, response, question, answer, context in zip(ids_, responses, questions_, answers_, contexts_):\n",
    "\n",
    "                                    response = response or \"No answer\"\n",
    "\n",
    "                                    em = int(exact_match_score(prediction=response, ground_truth=answer))\n",
    "                                    f1, _, _ = f1_score(prediction=response, ground_truth=answer)\n",
    "\n",
    "                                    evaluation = dict(\n",
    "                                        result=dict(response=response, EM=em, F1=f1),\n",
    "                                        instance=dict(id=id_, question=question, answer=answer)\n",
    "                                    )\n",
    "                                    std_result[index_strategy_desc][str(len(context))][id_] = evaluation\n",
    "\n",
    "                                    pbar.update()\n",
    "\n",
    "                            if (batch+1) % SAVE_STEPS == 0:\n",
    "                                std_result.save()\n",
    "                                common.sync_vram()\n",
    "\n",
    "                    std_result.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = {\n",
    "    'LLaMa-2-Chat-7B': '\\cite{touvron2023llama}',\n",
    "    'LLaMa-3-Chat-8B': '\\cite{metaIntroducingMeta}',\n",
    "    'LLaMa-2-Chat-13B': '\\cite{touvron2023llama}',\n",
    "    'Mistral-Instruct-7B-v2': '\\cite{jiang2023mistral}',\n",
    "    'Zephyr-SFT-7B': '\\cite{tunstall2023zephyr}',\n",
    "    'Zephyr-Beta-7B': '\\cite{tunstall2023zephyr}',\n",
    "    'GPT-3.5-U': '\\cite{ouyang2022training}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrrr}\n",
      "\\toprule\n",
      "model & index_type & chunk_size & EM_2 & F1_2 & EM_5 & F1_5 \\\\\n",
      "\\midrule\n",
      "LLaMa-2-Chat-7B \\cite{touvron2023llama} & basic & 100 & 0.443000 & 0.554317 & 0.509000 & 0.620906 \\\\\n",
      "LLaMa-3-Chat-8B \\cite{metaIntroducingMeta} & basic & 100 & 0.136000 & 0.337497 & 0.157000 & 0.382290 \\\\\n",
      "LLaMa-2-Chat-13B \\cite{touvron2023llama} & basic & 100 & 0.330000 & 0.438775 & 0.391000 & 0.493859 \\\\\n",
      "Mistral-Instruct-7B-v2 \\cite{jiang2023mistral} & basic & 100 & 0.064000 & 0.299703 & 0.091000 & 0.350969 \\\\\n",
      "Zephyr-SFT-7B \\cite{tunstall2023zephyr} & basic & 100 & 0.116000 & 0.278681 & 0.128000 & 0.305514 \\\\\n",
      "Zephyr-Beta-7B \\cite{tunstall2023zephyr} & basic & 100 & 0.013000 & 0.255154 & 0.035000 & 0.303718 \\\\\n",
      "GPT-3.5-U \\cite{ouyang2022training} & basic & 100 & 0.471000 & 0.566002 & 0.543000 & 0.652472 \\\\\n",
      "LLaMa-2-Chat-7B \\cite{touvron2023llama} & basic & 250 & 0.442000 & 0.563535 & 0.534000 & 0.647424 \\\\\n",
      "LLaMa-3-Chat-8B \\cite{metaIntroducingMeta} & basic & 250 & 0.147000 & 0.353004 & 0.153000 & 0.378661 \\\\\n",
      "LLaMa-2-Chat-13B \\cite{touvron2023llama} & basic & 250 & 0.318000 & 0.428672 & 0.404000 & 0.520729 \\\\\n",
      "Mistral-Instruct-7B-v2 \\cite{jiang2023mistral} & basic & 250 & 0.067000 & 0.312878 & 0.102000 & 0.364666 \\\\\n",
      "Zephyr-SFT-7B \\cite{tunstall2023zephyr} & basic & 250 & 0.121000 & 0.282489 & 0.131000 & 0.295879 \\\\\n",
      "Zephyr-Beta-7B \\cite{tunstall2023zephyr} & basic & 250 & 0.018000 & 0.262209 & 0.036000 & 0.306280 \\\\\n",
      "GPT-3.5-U \\cite{ouyang2022training} & basic & 250 & 0.456000 & 0.562436 & 0.565000 & 0.683145 \\\\\n",
      "LLaMa-2-Chat-7B \\cite{touvron2023llama} & semantic &  & 0.470000 & 0.580596 & 0.557000 & 0.670651 \\\\\n",
      "LLaMa-3-Chat-8B \\cite{metaIntroducingMeta} & semantic &  & 0.149000 & 0.362430 & 0.218000 & 0.437436 \\\\\n",
      "LLaMa-2-Chat-13B \\cite{touvron2023llama} & semantic &  & 0.336000 & 0.444464 & 0.433000 & 0.540371 \\\\\n",
      "Mistral-Instruct-7B-v2 \\cite{jiang2023mistral} & semantic &  & 0.080000 & 0.332133 & 0.117000 & 0.376279 \\\\\n",
      "Zephyr-SFT-7B \\cite{tunstall2023zephyr} & semantic &  & 0.114000 & 0.277651 & 0.154000 & 0.324209 \\\\\n",
      "Zephyr-Beta-7B \\cite{tunstall2023zephyr} & semantic &  & 0.025000 & 0.263465 & 0.047000 & 0.322649 \\\\\n",
      "GPT-3.5-U \\cite{ouyang2022training} & semantic &  & 0.496000 & 0.594398 & 0.608000 & 0.705538 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    records = []\n",
    "\n",
    "    for llm_model in MODELS:\n",
    "\n",
    "        std_result = data.NestedListItemResult(f\"results/counterfactual-post-hoc/{'-'.join(dataset)}_{llm_model}.json\")\n",
    "\n",
    "        for indexing_strategy in INDEXING:\n",
    "            indexing_strategy_desc = ','.join(str(term) for term in indexing_strategy)\n",
    "\n",
    "            records.append({\n",
    "                'model': llm_model + \" \" + citations[llm_model],\n",
    "                'index_type': indexing_strategy[0],\n",
    "                'chunk_size': indexing_strategy[-1] if len(indexing_strategy) == 3 else '',\n",
    "            })\n",
    "\n",
    "            for k in RETRIEVAL_DOCUMENTS:\n",
    "\n",
    "                avg_em, avg_f1, count = 0, 0, 0\n",
    "                for record in std_result[indexing_strategy_desc][str(k)].values():\n",
    "                    if record is not None:\n",
    "                        avg_em += record['result']['EM']\n",
    "                        avg_f1 += record['result']['F1']\n",
    "                        count += 1\n",
    "\n",
    "                avg_em /= (count or 1)\n",
    "                avg_f1 /= (count or 1)\n",
    "\n",
    "                records[-1].update({ f'EM_{k}': avg_em, f'F1_{k}': avg_f1 })\n",
    "\n",
    "print(pandas.DataFrame.from_records(records).sort_values(by=[ 'index_type', 'chunk_size' ]).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrr}\n",
      "\\toprule\n",
      "model & index_type & chunk_size & noise_robustness & faithfulness_2 & faithfulness_5 \\\\\n",
      "\\midrule\n",
      "LLaMa-2-Chat-7B & basic & 100 & 0.770909 & 0.682203 & 0.738532 \\\\\n",
      "LLaMa-3-Chat-8B & basic & 100 & 0.400000 & 0.292308 & 0.373134 \\\\\n",
      "LLaMa-2-Chat-13B & basic & 100 & 0.765060 & 0.560241 & 0.656805 \\\\\n",
      "Mistral-Instruct-7B-v2 & basic & 100 & 0.615385 & 0.384615 & 0.312500 \\\\\n",
      "Zephyr-SFT-7B & basic & 100 & 0.696429 & 0.535714 & 0.680000 \\\\\n",
      "Zephyr-Beta-7B & basic & 100 & 0.166667 & 0.166667 & 0.400000 \\\\\n",
      "GPT-3.5-U & basic & 100 & 0.867299 & 0.672986 & 0.697318 \\\\\n",
      "LLaMa-2-Chat-7B & basic & 250 & 0.763033 & 0.635071 & 0.762136 \\\\\n",
      "LLaMa-3-Chat-8B & basic & 250 & 0.272727 & 0.333333 & 0.270833 \\\\\n",
      "LLaMa-2-Chat-13B & basic & 250 & 0.737179 & 0.570513 & 0.677215 \\\\\n",
      "Mistral-Instruct-7B-v2 & basic & 250 & 0.655172 & 0.344828 & 0.358974 \\\\\n",
      "Zephyr-SFT-7B & basic & 250 & 0.727273 & 0.636364 & 0.608696 \\\\\n",
      "Zephyr-Beta-7B & basic & 250 & 0.500000 & 0.250000 & 0.400000 \\\\\n",
      "GPT-3.5-U & basic & 250 & 0.883065 & 0.629032 & 0.738182 \\\\\n",
      "LLaMa-2-Chat-7B & semantic &  & 0.754310 & 0.681034 & 0.744076 \\\\\n",
      "LLaMa-3-Chat-8B & semantic &  & 0.270270 & 0.297297 & 0.217391 \\\\\n",
      "LLaMa-2-Chat-13B & semantic &  & 0.754601 & 0.539877 & 0.648810 \\\\\n",
      "Mistral-Instruct-7B-v2 & semantic &  & 0.652174 & 0.565217 & 0.392857 \\\\\n",
      "Zephyr-SFT-7B & semantic &  & 0.714286 & 0.571429 & 0.647059 \\\\\n",
      "Zephyr-Beta-7B & semantic &  & 0.250000 & 0.000000 & 0.000000 \\\\\n",
      "GPT-3.5-U & semantic &  & 0.881818 & 0.659091 & 0.730337 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    records = []\n",
    "\n",
    "    for llm_model in MODELS:\n",
    "\n",
    "        std_result    = data.NestedListItemResult(f\"results/standard/{'-'.join(dataset)}_{llm_model}.json\")\n",
    "        ctrfct_result = data.NestedListItemResult(f\"results/counterfactual-post-hoc/{'-'.join(dataset)}_{llm_model}.json\")\n",
    "\n",
    "        for indexing_strategy in INDEXING:\n",
    "            indexing_strategy_desc = ','.join(str(term) for term in indexing_strategy)\n",
    "\n",
    "            common_count, std_count = 0, 0\n",
    "            index_results = std_result[indexing_strategy_desc]\n",
    "            for low_k_record, high_k_record in zip(index_results[str(RETRIEVAL_DOCUMENTS[0])].values(), index_results[str(RETRIEVAL_DOCUMENTS[-1])].values()):\n",
    "                if low_k_record is not None and high_k_record is not None:\n",
    "                    common_count += low_k_record['result']['EM'] * high_k_record['result']['EM']\n",
    "                    std_count += low_k_record['result']['EM']\n",
    "\n",
    "            noise_robustness = common_count / (std_count or 1)\n",
    "\n",
    "            records.append({\n",
    "                'model': llm_model,\n",
    "                'index_type': indexing_strategy[0],\n",
    "                'chunk_size': indexing_strategy[-1] if len(indexing_strategy) == 3 else '',\n",
    "                'noise_robustness': noise_robustness\n",
    "            })\n",
    "\n",
    "            for k in RETRIEVAL_DOCUMENTS:\n",
    "\n",
    "                common_count, std_count = 0, 0\n",
    "                for std_record, ctrfct_record in zip(std_result[indexing_strategy_desc][str(k)].values(), ctrfct_result[indexing_strategy_desc][str(k)].values()):\n",
    "                    if record is not None:\n",
    "                        common_count += std_record['result']['EM'] * ctrfct_record['result']['EM']\n",
    "                        std_count += std_record['result']['EM']\n",
    "\n",
    "                faithfulness = common_count / (std_count or 1)\n",
    "                records[-1].update({ f'faithfulness_{k}': faithfulness })\n",
    "\n",
    "\n",
    "print(pandas.DataFrame.from_records(records).sort_values(by=[ 'index_type', 'chunk_size' ]).to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
